{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trabalho de Conclusão de Curso\n",
    "\n",
    "##Aplicação de modelos de processamento de linguagem natural em um catálogo de vulnerabilidades cibernéticas\n",
    "\n",
    "###Identificação\n",
    "\n",
    "**Aluno:**  José Caetano Beuker\n",
    "\n",
    "**Curso:**  MBA em Data Science e Analytics\n",
    "\n",
    "**IE:** Escola Superior de Agricultura Luiz de Queiroz - Universidade de São Paulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para a manipulação de dataframes e análise de dados\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca para trabalhar com arrays grandes e multidimensionais e também matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Para visualizações\n",
    "import seaborn as sns\n",
    "\n",
    "# Para aprendizado de máquina\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Para regex - regular expression\n",
    "import re\n",
    "\n",
    "# Para nuvem de palavras\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Para visualização\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Toolkit - para PLN\n",
    "import nltk \n",
    "\n",
    "# from nltk import ngrams\n",
    "\n",
    "# Para remoção de stop words\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Atualiza a lista de stop words\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Cria objeto para remover stop words em Inglês\n",
    "stop_words_en = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar objeto para remover caracteres especiais\n",
    "remove_caracteres_especiais = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# Objeto para manter apenas caracteres alfanuméricos\n",
    "mantem_apenas_alfanumerico = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "# Função para limpar os textos das colunas do dataframe\n",
    "def limpa_texto(texto):\n",
    "    # Converte o texto para letras minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Subsitui caracteres por espaços \n",
    "    texto = remove_caracteres_especiais.sub(' ', texto)\n",
    "\n",
    "    # Remote caracteres que não forem alfanuméricos\n",
    "    texto = mantem_apenas_alfanumerico.sub('', texto)\n",
    "    \n",
    "    return texto \n",
    "\n",
    "# Função para tokenizar as palavras de uma coluna de um DataFrame\n",
    "def tokeniza_palavras_coluna (dataframe, coluna):\n",
    "    palavras = ' '.join([word for word in dataframe[coluna]])\n",
    "    # Realiza a tokenização\n",
    "    tokens = word_tokenize(palavras)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Função para preparar exibição de gráfico de ocorrências de palavras em uma coluna de um DataFrame\n",
    "def prepara_para_grafico (dataframe, coluna):\n",
    "    # Realiza a tokenização\n",
    "    tokens = tokeniza_palavras_coluna(dataframe, coluna)\n",
    "    # Obtém a frequência de ocorrências do token\n",
    "    frequencia = nltk.FreqDist(tokens)\n",
    "    pd_frequencia = pd.DataFrame({\"token\": list(frequencia.keys()),\n",
    "                                 \"frequencia\": list(frequencia.values())})\n",
    "    return pd_frequencia\n",
    "\n",
    "# Função para exibir gráfico de Pareto de ocorrências de palavras em uma coluna de um DataFrame\n",
    "def exibe_pareto (dados, coluna, quantidade_colunas, cor, titulo):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    x = sns.barplot(data = dados.nlargest(columns = coluna, n = quantidade_colunas), x = \"token\", y = \"frequencia\", color = cor)\n",
    "    x.set(ylabel = \"Quantidade\", xlabel = \"Tokens\", title = titulo)\n",
    "    plt.show()\n",
    "\n",
    "# Função para remover stop words\n",
    "def remove_stop_words(word_list, stop_word_list):\n",
    "    tokens = word_tokenize(word_list)\n",
    "    lista_sem_stop_words = ' '.join([word for word in tokens \n",
    "                                     if word not in stop_word_list])\n",
    "    return lista_sem_stop_words\n",
    "\n",
    "# Função de Stemming\n",
    "def stemming (texto):\n",
    "    stemmings = [porter.stem(word) for word in word_tokenize(texto)]\n",
    "    return ' '.join(stemmings)\n",
    "\n",
    "# Função de Lematização\n",
    "def lematizacao (texto):\n",
    "    lematizados = []\n",
    "    for word in texto:\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        lematizado = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "        lematizados.append(lematizado)\n",
    "    return lematizados    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar teste de cada função\n",
    "\n",
    "#Testa função limpa_texto\n",
    "def limpa_texto_teste():\n",
    "    \n",
    "    # texto sem limpar\n",
    "    texto_exemplo_sujo = [\"Let's study Data Science e Analytics at Escola Superior de Agricultura 'Luiz de Queiros', which is in Piracicaba/SP - Brasil.\"]\n",
    "    \n",
    "    # texto apos limpeza\n",
    "    texto_limpo = [\"lets study data science e analytics at escola superior de agricultura luiz de queiros  which is in piracicaba sp  brasil\"]\n",
    "\n",
    "    # Faz a comparação\n",
    "    for exemplo, resposta in zip(texto_exemplo_sujo, texto_limpo):\n",
    "\n",
    "        # Retorna resultado da comparação\n",
    "        if limpa_texto(exemplo) == resposta:\n",
    "            return \"Teste aprovado.\"\n",
    "        else:\n",
    "            raise Exception(\"Resposta não esperada em: '%s' \" % exemplo)\n",
    "    \n",
    "# Executando a função de teste\n",
    "try:\n",
    "    limpa_texto_teste()\n",
    "except NameError as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coleta de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta dados\n",
    "catalogoVulnerabilidades_bruto = pd.read_csv('https://www.cisa.gov/sites/default/files/csv/known_exploited_vulnerabilities.csv', header=0, sep=',')\n",
    "#catalogoVulnerabilidades = pd.read_csv('dados/known_exploited_vulnerabilities.csv', header=0, sep=',')\n",
    "\n",
    "# Mantém o DataFrame obtido originalmente sem alterações\n",
    "catalogoVulnerabilidades = catalogoVulnerabilidades_bruto.copy()\n",
    "\n",
    "# Criar coluna booleana state com 0 para Unknown e 1 para Known\n",
    "catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'] = np.where(catalogoVulnerabilidades['knownRansomwareCampaignUse'] == 'Known', 1, 0 )\n",
    "\n",
    "#Reordenando as colunas\n",
    "catalogoVulnerabilidades = catalogoVulnerabilidades[['cveID',\n",
    "                                                     'vendorProject',\n",
    "                                                     'product',\n",
    "                                                     'vulnerabilityName',\n",
    "                                                     'dateAdded',\n",
    "                                                     'shortDescription',\n",
    "                                                     'requiredAction',\n",
    "                                                     'dueDate',\n",
    "                                                     'knownRansomwareCampaignUse',\n",
    "                                                     'codigoKnownRansomwareCampaignUse',\n",
    "                                                     'notes']]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de colunas\n",
    "catalogoVulnerabilidades.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir a coluna notes\n",
    "catalogoVulnerabilidades.drop(columns=['notes'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar alguns dados\n",
    "display(catalogoVulnerabilidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar tipos de dados para datas\n",
    "catalogoVulnerabilidades.dateAdded = pd.to_datetime(catalogoVulnerabilidades.dateAdded)\n",
    "catalogoVulnerabilidades.dueDate = pd.to_datetime(catalogoVulnerabilidades.dueDate)\n",
    "\n",
    "##Tranforma a coluna codigoKnownRansomwareCampaignUse para boolean\n",
    "catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'] = catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'].astype('bool')\n",
    "\n",
    "##Transforma colunas em string\n",
    "catalogoVulnerabilidades[['shortDescription', \n",
    "                          'vulnerabilityName',\n",
    "                          'requiredAction']] = catalogoVulnerabilidades[['shortDescription', \n",
    "                                                                         'vulnerabilityName', \n",
    "                                                                         'requiredAction']].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de colunas após transformação\n",
    "catalogoVulnerabilidades.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerabilidade mais antiga catalogada\n",
    "mais_antiga = catalogoVulnerabilidades.sort_values(by='dateAdded')\n",
    "mais_antiga = mais_antiga.iloc[0]\n",
    "mais_antiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerabilidade mais recente catalogada\n",
    "mais_recente = catalogoVulnerabilidades.sort_values(by='dateAdded', ascending=False)\n",
    "mais_recente = mais_recente.iloc[0]\n",
    "mais_recente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informar coluna a ser analisada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coluna_em_analise = input('Informe o nome da coluna a ser analisada: ')\n",
    "\n",
    "coluna_em_analise = 'shortDescription'\n",
    "\n",
    "print(coluna_em_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame com pontos e caracteres especiais\n",
    "pd_frequencia_com_pontos = prepara_para_grafico(catalogoVulnerabilidades, coluna_em_analise)\n",
    "\n",
    "pd_frequencia_com_pontos.nlargest(columns = \"frequencia\", n = 10).sort_values(by=['frequencia'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe gráfico de pareto contendo caracteres especiais e stop words\n",
    "exibe_pareto(pd_frequencia_com_pontos, 'frequencia', 10, 'purple',  'Coluna ' + coluna_em_analise + ' contendo caracteres especiais e stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando as palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa a coluna \n",
    "pd_catalogoVulnerabilidades_coluna_limpa = pd.DataFrame(catalogoVulnerabilidades)\n",
    "pd_catalogoVulnerabilidades_coluna_limpa[coluna_em_analise] = catalogoVulnerabilidades[coluna_em_analise].apply(lambda x: limpa_texto(str(x)).lower())\n",
    "pd_catalogoVulnerabilidades_coluna_limpa[coluna_em_analise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza DataFrame limpo\n",
    "tokens_coluna_limpa = tokeniza_palavras_coluna(pd_catalogoVulnerabilidades_coluna_limpa, coluna_em_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega lista de stop words do inglês e acrescenta algumas palavras\n",
    "sw_vulnerabilidade_ciberneticas = [\"could\", \"vulnerability\", \"would\"] # Ver quais stop words adicionar\n",
    "sw_en = list(set(stopwords.words('english')))\n",
    "sw_en_plus = sw_en + sw_vulnerabilidade_ciberneticas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as stop words\n",
    "pd_catalogoVulnerabilidades_sem_stop_words = pd.DataFrame(pd_catalogoVulnerabilidades_coluna_limpa)\n",
    "pd_catalogoVulnerabilidades_sem_stop_words[coluna_em_analise] = pd_catalogoVulnerabilidades_sem_stop_words[coluna_em_analise].apply(lambda x: remove_stop_words(x, sw_en_plus))\n",
    "pd_catalogoVulnerabilidades_sem_stop_words[coluna_em_analise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria DataFrame sem stop words\n",
    "pd_frequencia_sem_stop_words = prepara_para_grafico(pd_catalogoVulnerabilidades_sem_stop_words, coluna_em_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe gráfico de pareto sem caracteres especiais e sem stop words\n",
    "exibe_pareto(pd_frequencia_sem_stop_words, 'frequencia', 10, 'blue',  'Coluna ' + coluna_em_analise + ' sem stop words e caracteres especiais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Stemming\n",
    "pd_catalogoVulnerabilidades_stemming = pd_catalogoVulnerabilidades_sem_stop_words.copy()\n",
    "pd_catalogoVulnerabilidades_stemming[coluna_em_analise] = pd_catalogoVulnerabilidades_stemming[coluna_em_analise].apply(lambda x: stemming(str(x)))\n",
    "pd_catalogoVulnerabilidades_stemming[coluna_em_analise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara para exibição de gráfico com stemming\n",
    "pd_frequencia_com_stemming = prepara_para_grafico(pd_catalogoVulnerabilidades_stemming, coluna_em_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pareto depois do stemming\n",
    "exibe_pareto(pd_frequencia_com_stemming, 'frequencia', 10, 'yellow',  'Coluna ' + coluna_em_analise + ' Stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Lemmatization\n",
    "pd_catalogoVulnerabilidades_lemmatization = pd_catalogoVulnerabilidades_sem_stop_words.copy()\n",
    "pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise] = lematizacao(pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise])\n",
    "\n",
    "pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara para exibição de gráfico com lemmatization\n",
    "df_frequencia_lemmatization = prepara_para_grafico(pd_catalogoVulnerabilidades_lemmatization, coluna_em_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras que mais ocorreram após a lemmatização\n",
    "df_frequencia_lemmatization.nlargest(columns = \"frequencia\", n = len(df_frequencia_lemmatization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras que menos ocorreram após a lemmatização\n",
    "df_frequencia_lemmatization.nsmallest(columns = \"frequencia\", n = len(df_frequencia_lemmatization)).sort_values(by=['frequencia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens com mais de 100 ocorrências\n",
    "\n",
    "df_frequencia_lemmatization_qtde = df_frequencia_lemmatization[(df_frequencia_lemmatization.frequencia >= 100)].sort_values(by = ['frequencia'], ascending=False)\n",
    "\n",
    "df_frequencia_lemmatization_qtde.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pareto depois do Lemmatization\n",
    "exibe_pareto(df_frequencia_lemmatization_qtde, 'frequencia', 10, 'green', 'Coluna ' + coluna_em_analise + ' após lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar coluna e criar a bag of words (sacola de palavras)\n",
    "vetorizar = CountVectorizer(lowercase = False, max_features=2731)\n",
    "bow = vetorizar.fit_transform(pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise])\n",
    "\n",
    "\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforma matriz esparsa em Dataframe\n",
    "coluna_em_analise_bow = pd.DataFrame.sparse.from_spmatrix(bow, columns=vetorizar.get_feature_names_out())\n",
    "\n",
    "coluna_em_analise_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "coluna_em_analise_all_words = ' '.join([word for word in pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise]])\n",
    "\n",
    "# Quantidade de palavras\n",
    "print(len(coluna_em_analise_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo bag of words\n",
    "coluna_em_analise_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar wordcloud\n",
    "\n",
    "## https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "\n",
    "coluna_em_analise_wc = WordCloud(width= 800, height= 500, max_font_size = 110, collocations=False).generate(coluna_em_analise_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar wordcloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(coluna_em_analise_wc, interpolation='bilinear') #ver outras interpolações\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "## Exemplo. Analisar a aplicação correta\n",
    "tfidf = TfidfVectorizer(lowercase = False, max_features=200)\n",
    "\n",
    "tfidf_catalogoVulnerabilidades_matriz = tfidf.fit_transform(pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise])\n",
    "\n",
    "tfidf_catalogoVulnerabilidades_df = pd.DataFrame(tfidf_catalogoVulnerabilidades_matriz.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "tfidf_catalogoVulnerabilidades_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngrams\n",
    "\n",
    "## Exemplo. Analisar a aplicação correta\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=200, ngram_range=(1,2))\n",
    "\n",
    "tfidf_catalogoVulnerabilidades_matriz_ngrams = tfidf.fit_transform(pd_catalogoVulnerabilidades_lemmatization[coluna_em_analise])\n",
    "\n",
    "tfidf_catalogoVulnerabilidades_df_ngrams = pd.DataFrame(tfidf_catalogoVulnerabilidades_matriz_ngrams.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "tfidf_catalogoVulnerabilidades_df_ngrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means\n",
    "\n",
    "# Criando um vetorizador TF-IDF para transformar as descrições em vetores numéricos\n",
    "coluna_analisada = tfidf_vectorizer.fit_transform(catalogoVulnerabilidades[coluna_em_analise])\n",
    "\n",
    "# Definindo a quantidade de clusters\n",
    "qtde_clusters = 7\n",
    "\n",
    "# Aplicando o algoritmo k-means\n",
    "kmeans = KMeans(qtde_clusters)\n",
    "kmeans.fit(coluna_analisada)\n",
    "\n",
    "# Adicionando os rótulos ao Dataframe\n",
    "catalogoVulnerabilidades['cluster'] = kmeans.labels_\n",
    "\n",
    "# Exibindo os clusters e suas descrições\n",
    "for cluster_id in range(qtde_clusters):\n",
    "    clusters_coluna_analisada = catalogoVulnerabilidades[catalogoVulnerabilidades['cluster'] == cluster_id][coluna_em_analise].values\n",
    "    print(f\"Cluster {cluster_id + 1}:\")\n",
    "    for descricao in clusters_coluna_analisada:\n",
    "        print(\"-\", descricao)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
