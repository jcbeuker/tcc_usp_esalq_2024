{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trabalho de Conclusão de Curso\n",
    "\n",
    "##Aplicação de modelos de processamento de linguagem natural em um catálogo de vulnerabilidades cibernéticas\n",
    "\n",
    "###Identificação\n",
    "\n",
    "**Aluno:**  José Caetano Beuker\n",
    "\n",
    "**Curso:**  MBA em Data Science e Analytics\n",
    "\n",
    "**IE:** Escola Superior de Agricultura Luiz de Queiroz - Universidade de São Paulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para a manipulação de dataframes e análise de dados\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca para trabalhar com arrays grandes e multidimensionais e também matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Para visualizações\n",
    "import seaborn as sns\n",
    "\n",
    "# Para aprendizado de máquina\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Para regex - regular expression\n",
    "import re\n",
    "\n",
    "# Para nuvem de palavras\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Para visualização\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Toolkit - para PLN\n",
    "import nltk \n",
    "\n",
    "# from nltk import ngrams\n",
    "\n",
    "# Para remoção de stop words\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Atualiza a lista de stop words\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Cria objeto para remover stop words em Inglês\n",
    "stop_words_en = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar objeto para remover caracteres especiais\n",
    "remove_caracteres_especiais = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# Objeto para manter apenas caracteres alfanuméricos\n",
    "mantem_apenas_alfanumerico = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "# Função para limpar os textos das colunas do dataframe\n",
    "def limpa_texto(texto):\n",
    "    # Converte o texto para letras minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Subsitui caracteres por espaços \n",
    "    texto = remove_caracteres_especiais.sub(' ', texto)\n",
    "\n",
    "    # Remote caracteres que não forem alfanuméricos\n",
    "    texto = mantem_apenas_alfanumerico.sub('', texto)\n",
    "    \n",
    "    return texto \n",
    "\n",
    "# Função para tokenizar as palavras de uma coluna de um DataFrame\n",
    "def tokeniza_palavras_coluna (dataframe, coluna):\n",
    "    palavras = ' '.join([word for word in dataframe[coluna]])\n",
    "    # Realiza a tokenização\n",
    "    tokens = word_tokenize(palavras)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Função para preparar exibição de gráfico de ocorrências de palavras em uma coluna de um DataFrame\n",
    "def prepara_para_grafico (dataframe, coluna):\n",
    "    # Realiza a tokenização\n",
    "    tokens = tokeniza_palavras_coluna(dataframe, coluna)\n",
    "    # Obtém a frequência de ocorrências do token\n",
    "    frequencia = nltk.FreqDist(tokens)\n",
    "    pd_frequencia = pd.DataFrame({\"token\": list(frequencia.keys()),\n",
    "                                 \"frequencia\": list(frequencia.values())})\n",
    "    return pd_frequencia\n",
    "\n",
    "# Função para exibir gráfico de Pareto de ocorrências de palavras em uma coluna de um DataFrame\n",
    "def exibe_pareto (dados, coluna, quantidade_colunas, cor, titulo):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = sns.barplot(data = dados.nlargest(columns = coluna, n = quantidade_colunas), x = \"token\", y = \"frequencia\", color = cor)\n",
    "    x.set(ylabel = \"Quantidade\", xlabel = \"Tokens\", title = titulo)\n",
    "    plt.show()\n",
    "\n",
    "# Função para remover stop words\n",
    "def remove_stop_words(word_list, stop_word_list):\n",
    "    tokens = word_tokenize(word_list)\n",
    "    lista_sem_stop_words = ' '.join([word for word in tokens \n",
    "                                     if word not in stop_word_list])\n",
    "    return lista_sem_stop_words\n",
    "\n",
    "# Função de Stemming\n",
    "def stemming (texto):\n",
    "    stemmings = [porter.stem(word) for word in word_tokenize(texto)]\n",
    "    return ' '.join(stemmings)\n",
    "\n",
    "# Função de Lematização\n",
    "def lematizacao (texto):\n",
    "    lematizados = []\n",
    "    for word in texto:\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        lematizado = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "        lematizados.append(lematizado)\n",
    "    return lematizados    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar teste de cada função\n",
    "\n",
    "#Testa função limpa_texto\n",
    "def limpa_texto_teste():\n",
    "    \n",
    "    # texto sem limpar\n",
    "    texto_exemplo_sujo = [\"Let's study Data Science e Analytics at Escola Superior de Agricultura 'Luiz de Queiros', which is in Piracicaba/SP - Brasil.\"]\n",
    "    \n",
    "    # texto apos limpeza\n",
    "    texto_limpo = [\"lets study data science e analytics at escola superior de agricultura luiz de queiros  which is in piracicaba sp  brasil\"]\n",
    "\n",
    "    # Faz a comparação\n",
    "    for exemplo, resposta in zip(texto_exemplo_sujo, texto_limpo):\n",
    "\n",
    "        # Retorna resultado da comparação\n",
    "        if limpa_texto(exemplo) == resposta:\n",
    "            return \"Teste aprovado.\"\n",
    "        else:\n",
    "            raise Exception(\"Resposta não esperada em: '%s' \" % exemplo)\n",
    "    \n",
    "# Executando a função de teste\n",
    "try:\n",
    "    limpa_texto_teste()\n",
    "except NameError as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coleta de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta dados\n",
    "catalogoVulnerabilidades_bruto = pd.read_csv('https://www.cisa.gov/sites/default/files/csv/known_exploited_vulnerabilities.csv', header=0, sep=',')\n",
    "#catalogoVulnerabilidades = pd.read_csv('dados/known_exploited_vulnerabilities.csv', header=0, sep=',')\n",
    "\n",
    "# Mantém o DataFrame obtido originalmente sem alterações\n",
    "catalogoVulnerabilidades = catalogoVulnerabilidades_bruto.copy()\n",
    "\n",
    "# Criar coluna booleana state com 0 para Unknown e 1 para Known\n",
    "catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'] = np.where(catalogoVulnerabilidades['knownRansomwareCampaignUse'] == 'Known', 1, 0 )\n",
    "\n",
    "#Reordenando as colunas\n",
    "catalogoVulnerabilidades = catalogoVulnerabilidades[['cveID',\n",
    "                                                     'vendorProject',\n",
    "                                                     'product',\n",
    "                                                     'vulnerabilityName',\n",
    "                                                     'dateAdded',\n",
    "                                                     'shortDescription',\n",
    "                                                     'requiredAction',\n",
    "                                                     'dueDate',\n",
    "                                                     'knownRansomwareCampaignUse',\n",
    "                                                     'codigoKnownRansomwareCampaignUse',\n",
    "                                                     'notes']]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de colunas\n",
    "catalogoVulnerabilidades.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir a coluna notes\n",
    "catalogoVulnerabilidades.drop(columns=['cveID', 'notes'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar alguns dados\n",
    "display(catalogoVulnerabilidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar tipos de dados para datas\n",
    "catalogoVulnerabilidades.dateAdded = pd.to_datetime(catalogoVulnerabilidades.dateAdded)\n",
    "catalogoVulnerabilidades.dueDate = pd.to_datetime(catalogoVulnerabilidades.dueDate)\n",
    "\n",
    "##Tranforma a coluna codigoKnownRansomwareCampaignUse para boolean\n",
    "catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'] = catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'].astype('bool')\n",
    "\n",
    "##Transforma colunas em string\n",
    "catalogoVulnerabilidades['shortDescription'] = catalogoVulnerabilidades['shortDescription'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de colunas após transformação\n",
    "catalogoVulnerabilidades.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame com pontos e caracteres especiais\n",
    "pd_frequencia_com_pontos = prepara_para_grafico(catalogoVulnerabilidades, 'shortDescription')\n",
    "\n",
    "pd_frequencia_com_pontos.nlargest(columns = \"frequencia\", n = 10).sort_values(by=['frequencia'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe gráfico de pareto contendo caracteres especiais e stop words\n",
    "exibe_pareto(pd_frequencia_com_pontos, 'frequencia', 10, 'purple', 'Contendo caracteres especiais e stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa a coluna \n",
    "pd_catalogoVulnerabilidades_coluna_limpa = pd.DataFrame(catalogoVulnerabilidades)\n",
    "pd_catalogoVulnerabilidades_coluna_limpa['shortDescription'] = catalogoVulnerabilidades['shortDescription'].apply(lambda x: limpa_texto(str(x)).lower())\n",
    "pd_catalogoVulnerabilidades_coluna_limpa.shortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza DataFrame limpo\n",
    "tokens_coluna_limpa = tokeniza_palavras_coluna(pd_catalogoVulnerabilidades_coluna_limpa, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega lista de stop words do inglês e acrescenta algumas palavras\n",
    "sw_vulnerabilidade_ciberneticas = [\"could\", \"vulnerability\", \"would\"] # Ver quais stop words adicionar\n",
    "sw_en = list(set(stopwords.words('english')))\n",
    "sw_en_plus = sw_en + sw_vulnerabilidade_ciberneticas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as stop words\n",
    "pd_catalogoVulnerabilidades_sem_stop_words = pd.DataFrame(pd_catalogoVulnerabilidades_coluna_limpa)\n",
    "pd_catalogoVulnerabilidades_sem_stop_words['shortDescription'] = pd_catalogoVulnerabilidades_sem_stop_words['shortDescription'].apply(lambda x: remove_stop_words(x, sw_en_plus))\n",
    "pd_catalogoVulnerabilidades_sem_stop_words.shortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria DataFrame sem stop words\n",
    "pd_frequencia_sem_stop_words = prepara_para_grafico(pd_catalogoVulnerabilidades_sem_stop_words, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe gráfico de pareto sem caracteres especiais e sem stop words\n",
    "exibe_pareto(pd_frequencia_sem_stop_words, 'frequencia', 10, 'blue', 'Sem stop words e caracteres especiais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Stemming\n",
    "pd_catalogoVulnerabilidades_stemming = pd_catalogoVulnerabilidades_sem_stop_words.copy()\n",
    "pd_catalogoVulnerabilidades_stemming['shortDescription'] = pd_catalogoVulnerabilidades_stemming['shortDescription'].apply(lambda x: stemming(str(x)))\n",
    "pd_catalogoVulnerabilidades_stemming.shortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara para exibição de gráfico com stemming\n",
    "pd_frequencia_com_stemming = prepara_para_grafico(pd_catalogoVulnerabilidades_stemming, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pareto depois do stemming\n",
    "exibe_pareto(pd_frequencia_com_stemming, 'frequencia', 10, 'yellow', 'Stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Lemmatization\n",
    "pd_catalogoVulnerabilidades_lemmatization = pd_catalogoVulnerabilidades_sem_stop_words.copy()\n",
    "pd_catalogoVulnerabilidades_lemmatization['shortDescription'] = lematizacao(pd_catalogoVulnerabilidades_lemmatization['shortDescription'])\n",
    "\n",
    "pd_catalogoVulnerabilidades_lemmatization.shortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara para exibição de gráfico com lemmatization\n",
    "df_frequencia_lemmatization = prepara_para_grafico(pd_catalogoVulnerabilidades_lemmatization, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras que mais ocorreram após a lemmatização\n",
    "df_frequencia_lemmatization.nlargest(columns = \"frequencia\", n = len(df_frequencia_lemmatization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras que menos ocorreram após a lemmatização\n",
    "df_frequencia_lemmatization.nsmallest(columns = \"frequencia\", n = len(df_frequencia_lemmatization)).sort_values(by=['frequencia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens com mais de 100 ocorrências\n",
    "\n",
    "df_frequencia_lemmatization_qtde = df_frequencia_lemmatization[(df_frequencia_lemmatization.frequencia >= 100)].sort_values(by = ['frequencia'], ascending=False)\n",
    "\n",
    "df_frequencia_lemmatization_qtde.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pareto depois do Lemmatization\n",
    "exibe_pareto(df_frequencia_lemmatization_qtde, 'frequencia', 10, 'green', 'Após lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar coluna e criar a bag of words (sacola de palavras)\n",
    "#vetorizar = CountVectorizer(lowercase = False, max_features=2731)\n",
    "vetorizar = CountVectorizer(lowercase = False, max_features=2300)\n",
    "bow = vetorizar.fit_transform(pd_catalogoVulnerabilidades_lemmatization.shortDescription)\n",
    "\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforma matriz esparsa em Dataframe\n",
    "df_shortDescription_bow = pd.DataFrame.sparse.from_spmatrix(bow, columns=vetorizar.get_feature_names_out())\n",
    "\n",
    "df_shortDescription_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "shortDescription_all_words = ' '.join([word for word in pd_catalogoVulnerabilidades_lemmatization.shortDescription])\n",
    "\n",
    "# Quantidade de palavras\n",
    "print(len(shortDescription_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo bag of words\n",
    "shortDescription_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar wordcloud\n",
    "\n",
    "## https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "\n",
    "shortDescription_wc = WordCloud(width= 800, height= 500, max_font_size = 110, collocations=False).generate(shortDescription_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar wordcloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(shortDescription_wc, interpolation='bilinear') #ver outras interpolações\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "## Exemplo. Analisar a aplicação correta\n",
    "tfidf = TfidfVectorizer(lowercase = False, max_features=200)\n",
    "regressao_logistica = LogisticRegression(solver = \"lbfgs\")\n",
    "\n",
    "catalogoVulnerabilidades_tfidf_bruto = tfidf.fit_transform(pd_catalogoVulnerabilidades_lemmatization['shortDescription'])\n",
    "\n",
    "catalogoVulnerabilidades_treino, catalogoVulnerabilidades_teste, catalogoVulnerabilidades_classe_treino, catalogoVulnerabilidades_classe_teste = train_test_split(catalogoVulnerabilidades_tfidf_bruto,\n",
    "                                                                                                                                                                  pd_catalogoVulnerabilidades_lemmatization['knownRansomwareCampaignUse'], random_state = 42)\n",
    "\n",
    "regressao_logistica.fit(catalogoVulnerabilidades_treino, catalogoVulnerabilidades_classe_treino)\n",
    "\n",
    "acuracia_catalogoVulnerabilidades_tfidf_bruto = regressao_logistica.score(catalogoVulnerabilidades_teste, catalogoVulnerabilidades_classe_teste)\n",
    "\n",
    "print(acuracia_catalogoVulnerabilidades_tfidf_bruto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngrams\n",
    "\n",
    "## Exemplo. Analisar a aplicação correta\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range=(1,2))\n",
    "catalogoVulnerabilidades_vetor_tfidf = tfidf.fit_transform(pd_catalogoVulnerabilidades_lemmatization['shortDescription'])\n",
    "\n",
    "catalogoVulnerabilidades_vetor_tfidf_treino, catalogoVulnerabilidades_vetor_tfidf_teste, catalogoVulnerabilidades_vetor_tfidf_classe_treino, catalogoVulnerabilidades_vetor_tfidf_classe_teste = train_test_split(catalogoVulnerabilidades_vetor_tfidf,\n",
    "                                                                                                                                                                  pd_catalogoVulnerabilidades_lemmatization['knownRansomwareCampaignUse'], random_state = 42)\n",
    "\n",
    "regressao_logistica.fit(catalogoVulnerabilidades_vetor_tfidf_treino, catalogoVulnerabilidades_vetor_tfidf_classe_treino)\n",
    "\n",
    "acuracia_catalogoVulnerabilidades_vetor_tfidf_ngrams = regressao_logistica.score(catalogoVulnerabilidades_vetor_tfidf_teste, catalogoVulnerabilidades_vetor_tfidf_classe_teste)\n",
    "\n",
    "print(acuracia_catalogoVulnerabilidades_vetor_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesos de cada termo\n",
    "\n",
    "## Exemplo. Analisar a aplicação correta\n",
    "pesos_termo = pd.DataFrame(\n",
    "    regressao_logistica.coef_[0].T,\n",
    "    index = tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "pesos_termo.nlargest(12, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
