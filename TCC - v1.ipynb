{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trabalho de Conclusão de Curso\n",
    "\n",
    "##Aplicação de modelos de processamento de linguagem natural em um catálogo de vulnerabilidades cibernéticas\n",
    "\n",
    "###Identificação\n",
    "\n",
    "**Aluno:**  José Caetano Beuker\n",
    "\n",
    "**Curso:**  MBA em Data Science e Analytics\n",
    "\n",
    "**IE:** Escola Superior de Agricultura Luiz de Queiroz - Universidade de São Paulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off warnings for a cleaner look\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Para a manipulação de dataframes e análise de dados\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca para trabalhar com arrays grandes e multidimensionais e também matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Para visualizações\n",
    "import seaborn as sns\n",
    "\n",
    "# Para visualização\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Toolkit - para PLN\n",
    "import nltk \n",
    "\n",
    "# Dividi o texto em tokens (palavras individuais)\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Para remoção de stop words\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Atualiza a lista de stop words\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Cria objeto para remover stop words em Inglês\n",
    "stop_words_en = stopwords.words('english')\n",
    "\n",
    "# Para regex - regular expression\n",
    "import re\n",
    "\n",
    "# Fazer o stemming das palavras (redução de palavras ao seu radical)\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Fazer a lematização de palavras (redução de palavras à sua forma base)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Contagem de palavras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Cria matriz de TF-FDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Para nuvem de palavras\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Para PLN\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo\n",
    "we_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtem a media dos embeddings do texto\n",
    "def media_embedding(texto):\n",
    "    documento = we_nlp(texto)\n",
    "    return documento.vector\n",
    "\n",
    "# Função para tokenizar palavras\n",
    "def tokeniza_palavras_coluna (dataframe, coluna):\n",
    "    palavras = ' '.join([word for word in dataframe[coluna]])\n",
    "    # Realiza a tokenização\n",
    "    tokens = word_tokenize(palavras)\n",
    "    return tokens\n",
    "\n",
    "# Função para preparar exibição de gráfico de ocorrências de palavras em uma coluna de um DataFrame\n",
    "def prepara_para_grafico (dataframe, coluna):\n",
    "    # Realiza a tokenização\n",
    "    tokens = tokeniza_palavras_coluna(dataframe, coluna)\n",
    "    # Obtém a frequência de ocorrências do token\n",
    "    frequencia = nltk.FreqDist(tokens)\n",
    "    pd_frequencia = pd.DataFrame({\"token\": list(frequencia.keys()),\n",
    "                                 \"frequencia\": list(frequencia.values())})\n",
    "    return pd_frequencia\n",
    "\n",
    "# Função para exibir gráfico de Pareto de ocorrências de palavras em uma coluna de um DataFrame\n",
    "def exibe_pareto (dados, coluna, quantidade_colunas, cor, titulo):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    x = sns.barplot(data = dados.nlargest(columns = coluna, n = quantidade_colunas), x = \"token\", y = \"frequencia\", color = cor)\n",
    "    x.set(ylabel = \"Quantidade\", xlabel = \"Tokens\", title = titulo)\n",
    "    plt.show()\n",
    "\n",
    "# Objeto para manter apenas caracteres alfanuméricos\n",
    "# mantem_apenas_alfanumerico = re.compile('[^0-9a-z #+_]')\n",
    "mantem_apenas_alfanumerico = re.compile('[^a-z #+_]')\n",
    "\n",
    "# Criar objeto para remover caracteres especiais\n",
    "remove_caracteres_especiais = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# Função para limpar os textos das colunas do dataframe\n",
    "def limpa_texto(texto):\n",
    "    # Converte o texto para letras minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Subsitui caracteres por espaços \n",
    "    texto = remove_caracteres_especiais.sub(' ', texto)\n",
    "\n",
    "    # Remote caracteres que não forem alfanuméricos\n",
    "    texto = mantem_apenas_alfanumerico.sub('', texto)\n",
    "    \n",
    "    return texto \n",
    "\n",
    "# Função para remover stop words\n",
    "def remove_stop_words(word_list, stop_word_list):\n",
    "    tokens = word_tokenize(word_list)\n",
    "    lista_sem_stop_words = ' '.join([word for word in tokens \n",
    "                                     if word not in stop_word_list])\n",
    "    return lista_sem_stop_words\n",
    "\n",
    "# Função de Stemming\n",
    "def stemming (texto):\n",
    "    stemmings = [porter.stem(word) for word in word_tokenize(texto)]\n",
    "    return ' '.join(stemmings)\n",
    "\n",
    "# Função de Lematização\n",
    "def lematizacao (texto):\n",
    "    lematizados = []\n",
    "    for word in texto:\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        lematizado = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "        lematizados.append(lematizado)\n",
    "    return lematizados  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta dados\n",
    "catalogoVulnerabilidades_bruto = pd.read_csv('https://www.cisa.gov/sites/default/files/csv/known_exploited_vulnerabilities.csv', header=0, sep=',')\n",
    "#catalogoVulnerabilidades = pd.read_csv('dados/known_exploited_vulnerabilities.csv', header=0, sep=',')\n",
    "\n",
    "# Mantém o DataFrame obtido originalmente sem alterações\n",
    "catalogoVulnerabilidades = catalogoVulnerabilidades_bruto.copy()\n",
    "\n",
    "# Criar coluna booleana state com 0 para Unknown e 1 para Known\n",
    "catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'] = np.where(catalogoVulnerabilidades['knownRansomwareCampaignUse'] == 'Known', 1, 0 )\n",
    "\n",
    "#Reordenando as colunas\n",
    "catalogoVulnerabilidades = catalogoVulnerabilidades[['cveID',\n",
    "                                                     'vendorProject',\n",
    "                                                     'product',\n",
    "                                                     'vulnerabilityName',\n",
    "                                                     'dateAdded',\n",
    "                                                     'shortDescription',\n",
    "                                                     'requiredAction',\n",
    "                                                     'dueDate',\n",
    "                                                     'knownRansomwareCampaignUse',\n",
    "                                                     'codigoKnownRansomwareCampaignUse',\n",
    "                                                     'notes']]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de colunas\n",
    "catalogoVulnerabilidades.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir a coluna notes\n",
    "catalogoVulnerabilidades.drop(columns=['vendorProject', 'product', 'notes'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar alguns dados\n",
    "catalogoVulnerabilidades.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar tipos de dados para datas\n",
    "catalogoVulnerabilidades.dateAdded = pd.to_datetime(catalogoVulnerabilidades.dateAdded)\n",
    "catalogoVulnerabilidades.dueDate = pd.to_datetime(catalogoVulnerabilidades.dueDate)\n",
    "\n",
    "##Tranforma a coluna codigoKnownRansomwareCampaignUse para boolean\n",
    "catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'] = catalogoVulnerabilidades['codigoKnownRansomwareCampaignUse'].astype('bool')\n",
    "\n",
    "##Transforma colunas em string\n",
    "catalogoVulnerabilidades[['shortDescription', \n",
    "                          'vulnerabilityName',\n",
    "                          'requiredAction']] = catalogoVulnerabilidades[['shortDescription', \n",
    "                                                                         'vulnerabilityName', \n",
    "                                                                         'requiredAction']].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de colunas após transformação\n",
    "catalogoVulnerabilidades.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerabilidade mais antiga catalogada\n",
    "mais_antiga = catalogoVulnerabilidades.sort_values(by='dateAdded')\n",
    "mais_antiga = mais_antiga.iloc[0]\n",
    "mais_antiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerabilidade mais recente catalogada\n",
    "mais_recente = catalogoVulnerabilidades.sort_values(by='dateAdded', ascending=False)\n",
    "mais_recente = mais_recente.iloc[0]\n",
    "mais_recente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copia apenas a coluna de interesse\n",
    "# Descricao_curta = catalogoVulnerabilidades['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tansforma em dataframe\n",
    "# Descricao_curta = pd.DataFrame(Descricao_curta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descricao_curta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descricao_curta.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a coluna shortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame com pontos e caracteres especiais\n",
    "pd_frequencia_dados_brutos = prepara_para_grafico(catalogoVulnerabilidades, 'shortDescription')\n",
    "\n",
    "pd_frequencia_dados_brutos.nlargest(columns = \"frequencia\", n = 10).sort_values(by=['frequencia'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe gráfico de pareto contendo caracteres especiais e stop words\n",
    "exibe_pareto(pd_frequencia_dados_brutos, 'frequencia', 10, 'purple',  'Coluna shortDescription contendo caracteres especiais e stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando as palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_limpo = catalogoVulnerabilidades.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Limpa a coluna \n",
    "# df_descricao_curta_coluna_limpa = pd.DataFrame(Descricao_curta)\n",
    "# df_descricao_curta_coluna_limpa['shortDescription'] = Descricao_curta['shortDescription'].apply(lambda x: limpa_texto(str(x)).lower())\n",
    "# df_descricao_curta_coluna_limpa['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa a coluna\n",
    "catalogoVulnerabilidades_limpo['shortDescription'] = catalogoVulnerabilidades_limpo['shortDescription'].apply(lambda x: limpa_texto(str(x)).lower())\n",
    "catalogoVulnerabilidades_limpo['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza DataFrame limpo\n",
    "tokens_coluna_limpa = tokeniza_palavras_coluna(catalogoVulnerabilidades_limpo, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega lista de stop words do inglês e acrescenta algumas palavras\n",
    "sw_vulnerabilidade_ciberneticas = [\"could\", \"couldn't\", \"vulnerability\", \"would\", \"wouldn't\"] # Ver quais stop words adicionar\n",
    "sw_en = list(set(stopwords.words('english')))\n",
    "sw_en_plus = sw_en + sw_vulnerabilidade_ciberneticas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove as stop words\n",
    "# df_descricao_curta_sem_stop_words = pd.DataFrame(df_descricao_curta_coluna_limpa)\n",
    "# df_descricao_curta_sem_stop_words['shortDescription'] = df_descricao_curta_sem_stop_words['shortDescription'].apply(lambda x: remove_stop_words(x, sw_en_plus))\n",
    "# df_descricao_curta_sem_stop_words['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as stop words\n",
    "# df_descricao_curta_sem_stop_words = pd.DataFrame(df_descricao_curta_coluna_limpa)\n",
    "catalogoVulnerabilidades_limpo['shortDescription'] = catalogoVulnerabilidades_limpo['shortDescription'].apply(lambda x: remove_stop_words(x, sw_en_plus))\n",
    "catalogoVulnerabilidades_limpo['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria DataFrame sem stop words\n",
    "pd_frequencia_sem_stop_words = prepara_para_grafico(catalogoVulnerabilidades_limpo, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe gráfico de pareto sem caracteres especiais e sem stop words\n",
    "exibe_pareto(pd_frequencia_sem_stop_words, 'frequencia', 10, 'blue',  'Coluna shortDescription sem stop words e caracteres especiais')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_stemming = catalogoVulnerabilidades_limpo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Stemming\n",
    "catalogoVulnerabilidades_stemming['shortDescription'] = catalogoVulnerabilidades_stemming['shortDescription'].apply(lambda x: stemming(str(x)))\n",
    "catalogoVulnerabilidades_stemming['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara para exibição de gráfico com stemming\n",
    "df_descricao_curta_stemming = prepara_para_grafico(catalogoVulnerabilidades_stemming, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pareto depois do stemming\n",
    "exibe_pareto(df_descricao_curta_stemming, 'frequencia', 10, 'yellow',  'Coluna shortDescription Stemming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_lemmatization = catalogoVulnerabilidades_limpo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Lemmatization\n",
    "catalogoVulnerabilidades_lemmatization['shortDescription'] = lematizacao(catalogoVulnerabilidades_lemmatization['shortDescription'])\n",
    "\n",
    "catalogoVulnerabilidades_lemmatization['shortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara para exibição de gráfico com lemmatization\n",
    "df_frequencia_lemmatization = prepara_para_grafico(catalogoVulnerabilidades_lemmatization, 'shortDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras que mais ocorreram após a lemmatização\n",
    "df_frequencia_lemmatization.nlargest(columns = \"frequencia\", n = len(df_frequencia_lemmatization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras que menos ocorreram após a lemmatização\n",
    "df_frequencia_lemmatization.nsmallest(columns = \"frequencia\", n = len(df_frequencia_lemmatization)).sort_values(by=['frequencia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens com mais de 100 ocorrências\n",
    "\n",
    "df_frequencia_lemmatization_qtde = df_frequencia_lemmatization[(df_frequencia_lemmatization.frequencia >= 100)].sort_values(by = ['frequencia'], ascending=False)\n",
    "\n",
    "df_frequencia_lemmatization_qtde.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pareto depois do Lemmatization\n",
    "exibe_pareto(df_frequencia_lemmatization_qtde, 'frequencia', 10, 'green', 'Coluna shortDescription após lemmatization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_bow = catalogoVulnerabilidades_limpo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar coluna e criar a bag of words (sacola de palavras)\n",
    "vetorizar = CountVectorizer(lowercase = False, max_features=2450)\n",
    "bow = vetorizar.fit_transform(catalogoVulnerabilidades_bow['shortDescription'])\n",
    "\n",
    "\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforma matriz esparsa em Dataframe\n",
    "matriz_bow = pd.DataFrame.sparse.from_spmatrix(bow, columns=vetorizar.get_feature_names_out())\n",
    "\n",
    "matriz_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "coluna_em_analise_all_words = ' '.join([word for word in catalogoVulnerabilidades_limpo['shortDescription']])\n",
    "\n",
    "# Quantidade de palavras\n",
    "print(len(coluna_em_analise_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar wordcloud\n",
    "\n",
    "## https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "\n",
    "coluna_em_analise_wc = WordCloud(width= 800, height= 500, max_font_size = 110, collocations=False).generate(coluna_em_analise_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar wordcloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(coluna_em_analise_wc, interpolation='bilinear') #ver outras interpolações\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_we = catalogoVulnerabilidades_limpo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we_df = pd.DataFrame(df_descricao_curta_lemmatization)\n",
    "\n",
    "catalogoVulnerabilidades_we['embedding'] = catalogoVulnerabilidades_we['shortDescription'].apply(media_embedding)\n",
    "\n",
    "catalogoVulnerabilidades_we[['cveID','shortDescription', 'embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_we.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o vetorizador TF-IDF\n",
    "vetorizador = TfidfVectorizer(lowercase=False, max_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplique o vetorizador aos dados\n",
    "matriz_tfidf = vetorizador.fit_transform(catalogoVulnerabilidades_limpo[\"shortDescription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_descricao_curta = pd.DataFrame(matriz_tfidf.toarray(), columns=vetorizador.get_feature_names_out())\n",
    "\n",
    "tfidf_df_descricao_curta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o vetorizador Ngrams ( Considerando os bigramas)\n",
    "vetorizador = TfidfVectorizer(lowercase=False, max_features=300, ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplique o vetorizador aos dados\n",
    "matriz_ngrams = vetorizador.fit_transform(catalogoVulnerabilidades_limpo[\"shortDescription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_df_descricao_curta = pd.DataFrame(matriz_ngrams.toarray(), columns=vetorizador.get_feature_names_out())\n",
    "\n",
    "ngrams_df_descricao_curta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória dos Dados\n",
    "### Analisando a coluna shortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem\n",
    "### Agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_we.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogoVulnerabilidades_cve = catalogoVulnerabilidades_we[['cveID', 'embedding']]\n",
    "\n",
    "print(catalogoVulnerabilidades_cve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
